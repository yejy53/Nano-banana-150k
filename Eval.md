# Geneval++ Evaluation

## 1. Workflow Summary

1. Generate images using the prompts from `Geneval++.txt`.
2. Run `Eval-gpt-4.1-geneval++.py` with the required parameters.
3. Review the tag-wise and overall accuracy metrics in the output.


## 2. Image Generation

Use your image generation model to produce images based on the prompts in **Geneval++.txt**.  
Save each generated image with a filename corresponding to the **line number** in the prompt file:

```shell
1.jpg
2.jpg
3.jpg
...
```

## 3. Evaluation

The script `Eval-gpt-4.1-geneval++.py` calculates evaluation metrics for the generated images.

### Required Parameters

```shell
meta_path = Path("Geneval++.jsonl")  # Provided Geneval++ metadata
image_dir = Path("image")            # Directory containing generated images
output_path = Path("Output.json")    # File path for evaluation results
```

You will also need to provide your API key when running the evaluation.


## 4. Example Output

```shell
ğŸ“Š Tag-wise Accuracy Report:
ğŸŸ© Tag: color_attr             | Accuracy: 85.00% (34/40)
ğŸŸ© Tag: spatial_count_attr     | Accuracy: 62.50% (25/40)
ğŸŸ© Tag: color_spatial_attr     | Accuracy: 62.50% (25/40)
ğŸŸ© Tag: color_count_attr       | Accuracy: 75.00% (30/40)
ğŸŸ© Tag: multi_object_count_attr| Accuracy: 85.00% (34/40)
ğŸŸ© Tag: size_spatial_attr      | Accuracy: 77.50% (31/40)
ğŸŸ© Tag: counting               | Accuracy: 65.00% (26/40)

â­ Overall score (mean of tag accuracies): 73.21%
â„¹ï¸ Overall accuracy (all samples): 73.21%
```

---

# Imagine-Bench

## 1. Workflow Summary

1. Generate images using the prompts from `Imagine.txt`.
2. Run `Eval-gpt-4.1-Imagine.py` with the required parameters.
3. Review the evaluation results, including full JSON outputs and score summaries.


## 2. Image Generation

Use your image generation model to produce images based on the prompts in **Imagine.txt**.  
Save each generated image with a filename corresponding to the **line number** in the prompt file:

1.jpg
2.jpg
3.jpg
...



## 3. Evaluation

The script **Eval-gpt-4.1-Imagine.py** calculates evaluation metrics for the generated images.

### Command Example

python Eval-gpt-4.1-Imagine.py \
--json_path data.jsonl \
--image_dir images \
--output_dir results \
--api_key "sk-XX-key" \
--model "gpt-4.1-2025-04-14" \
--result_full full.json \
--result_scores scores.jsonl



## 4. Example Output

ğŸ“Š Tag-wise Accuracy Report:
ğŸŸ© Tag: fantasy_fulfillment    | Accuracy: 88.00% (44/50)
ğŸŸ© Tag: identity_preservation  | Accuracy: 82.00% (41/50)
ğŸŸ© Tag: aesthetic_quality      | Accuracy: 90.00% (45/50)

â­ Overall score (mean of tag accuracies): 86.67%
â„¹ï¸ Overall accuracy (all samples): 86.67%


