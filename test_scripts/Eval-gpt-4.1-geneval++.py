import json
import base64
from pathlib import Path
import openai
import os
from tqdm import tqdm
from PIL import Image
import io
import re
from collections import defaultdict

# âœ… OpenAI client
client = openai.OpenAI(
    api_key=os.getenv("OPENAI_API_KEY", "...")
)

# âœ… Paths
meta_path = Path("Geneval++.jsonl") #Provided by Geneval++.josnl
image_dir = Path("image") #The image path generated by the model
output_path = Path("Output.json") #Detailed path of the output evaluation results

# âœ… Fixed System Prompt
SYSTEM_PROMPT = """You are an expert image evaluator.

Your task is to determine whether the given image faithfully satisfies the visual instruction and the expectation checklist.

Follow these rules strictly:
1. The image must match **all** expectations, including:
   - Object classes
   - Counts of each object
   - Colors of each object
   - Spatial position within the image (e.g., "above", "below", based on real pixel position)
   - Size and relative scale of objects
2. The image must appear as a **natural, coherent, photo-like single image**.
   - Do NOT allow stylized images (e.g., cartoons, sketches, anime).
   - Do NOT allow collage-style or multi-panel images. Only one consistent, realistic scene is acceptable.
3. Be very strict and conservative in your judgment. 

Return your result as a JSON object using this format:
{
  "correct": 1 if the image fully satisfies all expectations, else 0,
  "reason": "You may explain in detail what is missing or incorrect"
}
"""

# âœ… Encode image to base64
def encode_image(image_path: Path) -> str:
    with Image.open(image_path) as img:
        img = img.convert("RGB")
        buffer = io.BytesIO()
        img.save(buffer, format="JPEG")
        return base64.b64encode(buffer.getvalue()).decode("utf-8")

# âœ… Extract JSON from model response
def extract_json_from_response(text: str):
    text = text.strip()
    text = re.sub(r"^```json\s*", "", text)
    text = re.sub(r"\s*```$", "", text)
    match = re.search(r"\{[\s\S]*\}", text)
    if match:
        return json.loads(match.group(0))
    else:
        raise ValueError("No JSON object found")

# âœ… Convert metadata to natural-language checklist
def metadata_to_explanation(metadata: dict) -> str:
    parts = []

    def format_item(item: dict) -> str:
        obj = item["class"]
        count = item.get("count", 1)
        color = item.get("color", None)
        region = item.get("region", None)
        size = item.get("size", None)
        noun = f"{count} {obj + 's' if count > 1 else obj}"
        desc_parts = []
        if color:
            desc_parts.append(f"{color}-colored")
        if size:
            desc_parts.append(size)
        if desc_parts:
            noun = f"{' '.join(desc_parts)} {noun}"
        if region:
            return f"{noun} located in the {region} part of the image"
        else:
            return f"{noun} present in the image"

    for item in metadata.get("include", []):
        parts.append(f"- {format_item(item)}.")
    for item in metadata.get("exclude", []):
        obj = item["class"]
        count = item.get("count", 1)
        noun = f"{obj + 's' if count > 1 else obj}"
        parts.append(f"- No more than {count - 1} {noun} should appear.")

    return "This image should contain:\n" + "\n".join(parts)

# âœ… Core per-sample processing (single item)
def process_sample(idx: int, metadata: dict, max_retries: int = 3) -> dict:
    instruction = metadata.get("prompt", "").strip()
    explanation = metadata_to_explanation(metadata)
    image_path = image_dir / f"{idx + 1}.jpg"

    if not image_path.exists():
        return {
            "id": idx + 1,
            "tag": metadata.get("tag", ""),
            "prompt": instruction,
            "result": {"correct": 0, "reason": "Image not found"}
        }

    image_b64 = encode_image(image_path)

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": [
            {"type": "text", "text": f"Instruction:\n{instruction}\n\nExpectation checklist:\n{explanation}"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"}}
        ]}
    ]

    for attempt in range(1, max_retries + 1):
        try:
            response = client.chat.completions.create(
                model="gpt-4.1",
                messages=messages,
                max_tokens=200,
                temperature=0.0
            )
            parsed = extract_json_from_response(response.choices[0].message.content)
            return {
                "id": idx + 1,
                "tag": metadata.get("tag", ""),
                "prompt": instruction,
                "result": parsed
            }

        except Exception as e:
            if attempt < max_retries:
                print(f"âš ï¸ Retry {attempt} failed for image {idx+1}: {e}")
                continue
            else:
                return {
                    "id": idx + 1,
                    "tag": metadata.get("tag", ""),
                    "prompt": instruction,
                    "result": {"correct": -1, "reason": f"Image not successfully evaluated after {max_retries} retries: {e}"}
                }


# âœ… Main (single-threaded evaluation)
def main():
    with meta_path.open("r") as f:
        lines = [json.loads(line.strip()) for line in f if line.strip()]

    results = []
    tag_counter = defaultdict(lambda: {"correct": 0, "total": 0})

    for idx, metadata in tqdm(enumerate(lines), total=len(lines), desc="Evaluating"):
        result = process_sample(idx, metadata)
        results.append(result)

        tag = result["tag"]
        tag_counter[tag]["total"] += 1
        if result["result"].get("correct", 0) == 1:
            tag_counter[tag]["correct"] += 1

    # Per-tag accuracy
    tag_accuracy = {
        tag: round(cnt["correct"] / cnt["total"], 4) if cnt["total"] > 0 else 0.0
        for tag, cnt in tag_counter.items()
    }

    # âœ… Overall score = mean of tag accuracies (as requested)
    if len(tag_accuracy) > 0:
        overall_score = round(sum(tag_accuracy.values()) / len(tag_accuracy), 4)
    else:
        overall_score = 0.0

    # (Optional) overall accuracy across all samples
    total_all = sum(v["total"] for v in tag_counter.values())
    correct_all = sum(v["correct"] for v in tag_counter.values())
    overall_accuracy = round(correct_all / total_all, 4) if total_all > 0 else 0.0

    print("\nðŸ“Š Tag-wise Accuracy Report:")
    for tag, acc in tag_accuracy.items():
        total = tag_counter[tag]["total"]
        correct = tag_counter[tag]["correct"]
        print(f"ðŸŸ© Tag: {tag:<20} | Accuracy: {acc*100:.2f}%  ({correct}/{total})")

    print(f"\nâ­ Overall score (mean of tag accuracies): {overall_score*100:.2f}%")
    print(f"Overall accuracy (all samples): {overall_accuracy*100:.2f}%")

    final_output = {
        "overall_results": results,
        "tag_accuracy": tag_accuracy,
        "overall_score_mean_of_tags": overall_score,   # requested overall score
        "overall_accuracy_all_samples": overall_accuracy
    }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w") as f:
        json.dump(final_output, f, indent=2)

    print(f"\nâœ… Finished. Total samples: {len(results)}. Results written to: {output_path}")


if __name__ == "__main__":
    main()
